# DATA-PIPELINE-DEVELOPMENT
# ğŸ”„ Data Pipeline Development â€“ Task 1 (CODTECH Internship)

Automated **ETL pipeline** built using **Pandas** and **Scikitâ€‘learn** during the CODTECH Data Science internship. This project demonstrates how to systematically **Extract, Preprocess, Transform**, and **Load** real-world data.

---

## ğŸ“Œ Project Overview

This ETL pipeline:
1. **Extracts** raw data from a CSV (e.g., chocolate sales).
2. **Preprocesses**:
   - Handles missing values (mode for categoricals, median for numerics).
   - Removes duplicates and parses dates.
   - Converts booleans to integers.
3. **Transforms**:
   - Label-encodes categorical features.
   - Scales numeric columns (e.g., `Qty`, `Amount`) using `StandardScaler`.
4. **Loads** the cleaned and transformed data to a new CSV file.

Inspired by real-world pipelines using Pandas and Scikit-learn :contentReference[oaicite:1]{index=1}.

---

## ğŸ—‚ï¸ Repository Structure
data-pipeline-task1/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw_data.csv
â”‚ â””â”€â”€ processed_data.csv
â”œâ”€â”€ etl_pipeline.ipynb # Notebook with full ETL workflow
â”œâ”€â”€ etl_pipeline.py # Script version (optional)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## âš™ï¸ Technologies Used

- Python 3.x  
- pandas  
- scikit-learn (LabelEncoder, StandardScaler)  
- joblib *(optional)* for saving pipelines

---

## ğŸš€ How to Run

1. **Clone the repository**:
   ```bash
   git clone https://github.com/yourusername/data-pipeline-task1.git
   cd data-pipeline-task1

2. **Install dependencies**:
   pip install -r requirements.txt
   
3. **Run the pipeline**:
   In Jupyter: open and run etl_pipeline.ipynb
   

ğŸ‘€ **Why It Matters**

1. Creates clean, consistent data for ML/analytics.
2. Reproducible pipelineâ€”run on new datasets with ease, avoids manual errors.
3. Establishes a scalable structure ready for deployment or integration.


Crafted by Nisha during the CODTECH internship
Mentor: Neela Santosh 
